{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'running': 'run', 'better': 'better', 'studies': 'study', 'wolves': 'wolves', 'mice': 'mice', 'children': 'children', 'was': 'be', 'ate': 'eat', 'swimming': 'swim', 'parties': 'party', 'leaves': 'leave', 'knives': 'knives', 'happier': 'happier', 'studying': 'study', 'played': 'play', 'goes': 'go', 'driving': 'drive', 'talked': 'talk'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"running\", \"better\", \"studies\", \"wolves\", \"mice\", \"children\", \"was\", \"ate\", \n",
    "         \"swimming\", \"parties\", \"leaves\", \"knives\", \"happier\", \"studying\", \"played\", \n",
    "         \"goes\", \"driving\", \"talked\"]\n",
    "\n",
    "lemmas = {word: lemmatizer.lemmatize(word, pos='v') for word in words}  \n",
    "print(lemmas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "frase limpa : the children were playing in the leaves yesterday\n",
      "Frase Token :  ['the', 'children', 'were', 'playing', 'in', 'the', 'leaves', 'yesterday']\n",
      "Frase remove stop:  ['children', 'playing', 'leaves', 'yesterday']\n",
      "frase lematizada:  ['child', 'playing', 'leaf', 'yesterday']\n",
      "==============================================\n",
      "\n",
      "==============================================\n",
      "frase limpa : she studies computer science and is taking three courses\n",
      "Frase Token :  ['she', 'studies', 'computer', 'science', 'and', 'is', 'taking', 'three', 'courses']\n",
      "Frase remove stop:  ['studies', 'computer', 'science', 'taking', 'three', 'courses']\n",
      "frase lematizada:  ['study', 'computer', 'science', 'taking', 'three', 'course']\n",
      "==============================================\n",
      "\n",
      "==============================================\n",
      "frase limpa : the wolves howled at the moon while mice scurried in the grass\n",
      "Frase Token :  ['the', 'wolves', 'howled', 'at', 'the', 'moon', 'while', 'mice', 'scurried', 'in', 'the', 'grass']\n",
      "Frase remove stop:  ['wolves', 'howled', 'moon', 'mice', 'scurried', 'grass']\n",
      "frase lematizada:  ['wolf', 'howled', 'moon', 'mouse', 'scurried', 'grass']\n",
      "==============================================\n",
      "\n",
      "==============================================\n",
      "frase limpa : he was driving faster than the cars around him\n",
      "Frase Token :  ['he', 'was', 'driving', 'faster', 'than', 'the', 'cars', 'around', 'him']\n",
      "Frase remove stop:  ['driving', 'faster', 'cars', 'around']\n",
      "frase lematizada:  ['driving', 'faster', 'car', 'around']\n",
      "==============================================\n",
      "\n",
      "==============================================\n",
      "frase limpa : the chefs used sharp knives to prepare the tastiest dishes\n",
      "Frase Token :  ['the', 'chefs', 'used', 'sharp', 'knives', 'to', 'prepare', 'the', 'tastiest', 'dishes']\n",
      "Frase remove stop:  ['chefs', 'used', 'sharp', 'knives', 'prepare', 'tastiest', 'dishes']\n",
      "frase lematizada:  ['chef', 'used', 'sharp', 'knife', 'prepare', 'tastiest', 'dish']\n",
      "==============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Baixar recursos necessários do NLTK\n",
    "\n",
    "\n",
    "def basic_cleaning(text):\n",
    "    # Converter para minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remover pontuações\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remover números\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remover espaços extras\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokeniza um texto dividindo por espaços\n",
    "    \"\"\"\n",
    "    return text.split()\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Remove stopwords da lista de tokens\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Lematiza uma lista de tokens usando WordNetLemmatizer do NLTK\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "frases = [\n",
    "    \"The children were playing in the leaves yesterday.\",\n",
    "    \"She studies computer science and is taking three courses.\",\n",
    "    \"The wolves howled at the moon while mice scurried in the grass.\",\n",
    "    \"He was driving faster than the cars around him.\",\n",
    "    \"The chefs used sharp knives to prepare the tastiest dishes.\"\n",
    "]\n",
    "for i in frases:\n",
    "    print(\"==============================================\")\n",
    "    \n",
    "    frase_limpa = basic_cleaning(i)\n",
    "    print (\"frase limpa : \" + frase_limpa)\n",
    "    \n",
    "    frase_token = simple_tokenize(frase_limpa)\n",
    "    print (\"Frase Token : \" , frase_token)\n",
    "\n",
    "    frase_sem_stop = remove_stopwords(frase_token)\n",
    "    print (\"Frase remove stop: \" , frase_sem_stop)\n",
    "\n",
    "    frase_lematizada = lemmatize_tokens(frase_sem_stop)\n",
    "    print (\"frase lematizada: \" , frase_lematizada)\n",
    "\n",
    "    print(\"==============================================\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
